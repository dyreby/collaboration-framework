# Philosophy

The formal grounding for those who want it. You don't need this to collaborate effectively with me—the [README](../README.md) is self-sufficient. This is the derivation behind that story.

## Axioms

A model encodes assumptions—invariants you take as given so you can reason about something more simply. The right assumptions let you create a simpler model that's actually useful. The wrong assumptions (or too many) make the model collapse under its own weight.

The simplest model I could find for collaboration starts here:

- Something exists.
- Distinct things exist (you and me, agent and world).
- Physics is consistent at the scale we operate.
- An agent is an entity capable of acting in the world.

These are models too, with their own assumptions. Non-duality questions whether distinct things exist. Quantum mechanics questions classical consistency. But you can model a pendulum without accounting for quantum physics, even though the pendulum operates in quantum reality. I'm choosing the level that lets me say something useful about collaboration, not claiming these are ultimately true.

## Propositions

From those axioms, six propositions apply to any agent—human or AI:

1. **Agents act from internal objectives.** This is intent.
2. **Agents have internal world models** that are incomplete, fallible, and mutable.
3. **Agents use their world models** to select actions in pursuit of objectives.
4. **World input is taken as given**; what it means is model-derived.
5. **Collaboration adds context** that agents can't derive alone, including the intent of the collaborator.
6. **Expressed intent is lossy.** Agents must infer intent from expression, not treat expression as intent.

That sixth proposition is where this all leads. Words are models of concepts, and models simplify. Under the assumptions above—all the way back to distinct things existing—this necessarily follows.

These aren't claims I'm asking you to accept on faith. They're the reasoning behind the README's claims. If they hold up for you, the framework will make more sense. If they don't, you can still collaborate with me—you just won't have the formal grounding for why it works.

## The Loop

Because expressed intent is lossy, collaboration needs a correction loop. OODA (Observe, Orient, Decide, Act) is that loop.

```
        ┌──────────────────────────────┐
        │                              │
        ▼                              │
    Observe → Orient → Decide → Act ───┘
                 ▲
                 │
            [concepts]
```

**Observe**: Notice what happened—feedback, friction, misalignment.

**Orient**: Interpret through your worldview—concepts, principles, mental models. This is where the framework lives.

**Decide**: Choose whether to update your understanding or proceed as-is.

**Act**: Make the change (or don't). The results become new observations.

The human is essential at Decide. Only I know if what I observed matches what I intended. An agent can propose, orient, even act—but the decision to accept the result or iterate again is mine. That's the invariant from The Gap (expression ≠ intent): I'm the one who closes the loop.
